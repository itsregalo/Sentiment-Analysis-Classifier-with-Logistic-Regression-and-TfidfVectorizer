{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use your own kaggle.json\n",
    "!mkdir ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "\n",
    "# change the permission\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# download dataset\n",
    "!kaggle datasets download -d krishbaisoya/tweets-sentiment-analysis\n",
    "\n",
    "# unzip the dataset\n",
    "!unzip tweets-sentiment-analysis.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use the data to train a model to predict the sentiment of a tweet, either positive or negative.\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "%matplotlib inline\n",
    "\n",
    "# load the data\n",
    "training_data = pd.read_csv('train_data.csv')\n",
    "testing_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "# check the data\n",
    "training_data.head()\n",
    "\n",
    "# check the data\n",
    "testing_data.head()\n",
    "\n",
    "# check the shape of the data\n",
    "print(\"Training data shape: \", training_data.shape)\n",
    "print(\"Testing data shape: \", testing_data.shape)\n",
    "\n",
    "# check the null values\n",
    "training_data.isnull().sum()\n",
    "\n",
    "# check the null values\n",
    "testing_data.isnull().sum()\n",
    "\n",
    "# unique values in the sentiment column\n",
    "training_data['Sentiment'].unique()\n",
    "\n",
    "# plot the distribution of sentiment labels in the training data\n",
    "sns.countplot(x='sentiment', data=training_data)\n",
    "plt.xlabel('Sentiment Label')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Sentiment Labels in Training Data')\n",
    "plt.xticks([0, 1], ['Negative', 'Positive'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plot the distribution of sentiment labels in the testing data\n",
    "sns.countplot(x='sentiment', data=testing_data)\n",
    "plt.xlabel('Sentiment Label')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Sentiment Labels in Testing Data')\n",
    "plt.xticks([0, 1], ['Negative', 'Positive'])\n",
    "plt.show()\n",
    "\n",
    "# preprocess the data\n",
    "training_data['sentence'] = training_data['sentence'].apply(lambda x: x.lower())\n",
    "training_data['sentence'] = training_data['sentence'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "\n",
    "testing_data['sentence'] = testing_data['sentence'].apply(lambda x: x.lower())\n",
    "testing_data['sentence'] = testing_data['sentence'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "\n",
    "# stopwords removal\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "punctuation = list(string.punctuation)\n",
    "\n",
    "\n",
    "# remove stopwords from the sentence\n",
    "def remove_stopwords(sentence):\n",
    "    sentence = sentence.split()\n",
    "    sentence = [word for word in sentence if word not in stop_words]\n",
    "    sentence = \" \".join(sentence)\n",
    "    return sentence\n",
    "\n",
    "# lowercase the sentence\n",
    "def lowercase(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    return sentence\n",
    "\n",
    "# remove punctuation from the sentence\n",
    "def remove_punctuation(sentence):\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    return sentence\n",
    "\n",
    "# remove numbers from the sentence\n",
    "def remove_numbers(sentence):\n",
    "    sentence = re.sub(r'\\d+', '', sentence)\n",
    "    return sentence\n",
    "\n",
    "# remove extra spaces from the sentence\n",
    "def remove_extra_spaces(sentence):\n",
    "    sentence = re.sub(' +', ' ', sentence)\n",
    "    return sentence\n",
    "\n",
    "# remove stopwords from the sentence\n",
    "def remove_stopwords(sentence):\n",
    "    sentence = sentence.split()\n",
    "    sentence = [word for word in sentence if word not in stop_words]\n",
    "    sentence = \" \".join(sentence)\n",
    "    return sentence\n",
    "\n",
    "# tokenize the sentence\n",
    "def tokenize(sentence):\n",
    "    tweet_tokenizer = nltk.tokenize.TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    sentence = tweet_tokenizer.tokenize(sentence)\n",
    "    sentence = \" \".join(sentence)\n",
    "    return sentence\n",
    "\n",
    "# preprocess the data\n",
    "training_data['sentence'] = training_data['sentence'].apply(lowercase)\n",
    "training_data['sentence'] = training_data['sentence'].apply(remove_punctuation)\n",
    "training_data['sentence'] = training_data['sentence'].apply(remove_numbers)\n",
    "training_data['sentence'] = training_data['sentence'].apply(remove_extra_spaces)\n",
    "training_data['sentence'] = training_data['sentence'].apply(remove_stopwords)\n",
    "training_data['sentence'] = training_data['sentence'].apply(tokenize)\n",
    "\n",
    "testing_data['sentence'] = testing_data['sentence'].apply(lowercase)\n",
    "testing_data['sentence'] = testing_data['sentence'].apply(remove_punctuation)\n",
    "testing_data['sentence'] = testing_data['sentence'].apply(remove_numbers)\n",
    "testing_data['sentence'] = testing_data['sentence'].apply(remove_extra_spaces)\n",
    "testing_data['sentence'] = testing_data['sentence'].apply(remove_stopwords)\n",
    "testing_data['sentence'] = testing_data['sentence'].apply(tokenize)\n",
    "\n",
    "\n",
    "# print before and after preprocessing\n",
    "print(\"Before preprocessing: \", training_data['sentence'][0])\n",
    "print(\"After preprocessing: \", training_data['sentence'][0])\n",
    "\n",
    "\n",
    "# split the data into train and validation set\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(training_data['sentence'], training_data['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# tokenize the data\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "train_x_tfidf = tfidf_vectorizer.fit_transform(train_x)\n",
    "valid_x_tfidf = tfidf_vectorizer.transform(valid_x)\n",
    "\n",
    "# train the model\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(train_x_tfidf, train_y)\n",
    "\n",
    "# predict the sentiment of the validation set\n",
    "predictions = logistic_regression.predict(valid_x_tfidf)\n",
    "\n",
    "# print the classification report\n",
    "print(classification_report(valid_y, predictions))\n",
    "\n",
    "# print the confusion matrix\n",
    "print(confusion_matrix(valid_y, predictions))\n",
    "\n",
    "\n",
    "# predict the sentiment of the testing set\n",
    "test_x_tfidf = tfidf_vectorizer.transform(testing_data['sentence'])\n",
    "predictions = logistic_regression.predict(test_x_tfidf)\n",
    "\n",
    "# print the classification report\n",
    "print(classification_report(testing_data['sentiment'], predictions))\n",
    "\n",
    "# save the model\n",
    "pickle.dump(logistic_regression, open('logistic_regression.pkl', 'wb'))\n",
    "pickle.dump(tfidf_vectorizer, open('tfidf_vectorizer.pkl', 'wb'))\n",
    "\n",
    "# load the model\n",
    "logistic_regression = pickle.load(open('logistic_regression.pkl', 'rb'))\n",
    "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
    "\n",
    "# predict the sentiment from the user input\n",
    "def predict_sentiment(user_input):\n",
    "    user_input = lowercase(user_input)\n",
    "    user_input = remove_punctuation(user_input)\n",
    "    user_input = remove_numbers(user_input)\n",
    "    user_input = remove_extra_spaces(user_input)\n",
    "    user_input = remove_stopwords(user_input)\n",
    "    user_input = tokenize(user_input)\n",
    "    user_input = tfidf_vectorizer.transform([user_input])\n",
    "    prediction = logistic_regression.predict(user_input)\n",
    "    return prediction\n",
    "\n",
    "# get the user input\n",
    "user_input = input(\"Enter a sentence: \")\n",
    "\n",
    "# predict the sentiment of the user input\n",
    "prediction = predict_sentiment(user_input)\n",
    "\n",
    "# print the sentiment\n",
    "if prediction == 0:\n",
    "    print(\"Negative\")\n",
    "else:\n",
    "    print(\"Positive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now i will mine data from twitter using tweepy and then i will predict the sentiment of the tweets\n",
    "\n",
    "# import the libraries\n",
    "#import dependencies\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "from unidecode import unidecode\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm \n",
    "from decouple import config\n",
    "\n",
    "# Authenticate to Twitter\n",
    "import tweepy\n",
    "\n",
    "\n",
    "# Authenticate to Twitter\n",
    "api_key= config('api_key')\n",
    "api_key_secret= config('api_key_secret')\n",
    "bearer_token= config('bearer_token')\n",
    "access_token= config('access_token')\n",
    "access_token_secret= config('access_token_secret')\n",
    "\n",
    "auth = tweepy.OAuthHandler(api_key, api_key_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "def tweetSearch(query, limit):\n",
    "    \"\"\"\n",
    "    This function will search a query provided in the twitter and,\n",
    "    retun a list of all tweets that have a query. \n",
    "    \"\"\"\n",
    "\n",
    "    # Create a blank variable\n",
    "    tweets = []\n",
    "\n",
    "    # Iterate through Twitter using Tweepy to find our query with our defined limit\n",
    "    for tweet in tweepy.Cursor(api.search, q=query, tweet_mode='extended').items(limit):\n",
    "        tweets.append(tweet)\n",
    "\n",
    "    return tweets\n",
    "\n",
    "\n",
    "def tweets_to_data_frame(tweets):\n",
    "    \"\"\"\n",
    "    This function will receive tweets and collect specific data from it such as place, tweet's text,likes \n",
    "    retweets and save them into a pandas data frame.\n",
    "    \n",
    "    This function will return a pandas data frame that contains data from twitter.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(data=[tweet.full_text.encode('utf-8') for tweet in tweets], columns=[\"Tweets\"])\n",
    "\n",
    "    df[\"id\"] = np.array([tweet.id for tweet in tweets])\n",
    "    df[\"lens\"] = np.array([len(tweet.full_text) for tweet in tweets])\n",
    "    df[\"date\"] = np.array([tweet.created_at for tweet in tweets])\n",
    "    df[\"place\"] = np.array([tweet.place for tweet in tweets])\n",
    "    df[\"coordinateS\"] = np.array([tweet.coordinates for tweet in tweets])\n",
    "    df[\"lang\"] = np.array([tweet.lang for tweet in tweets])\n",
    "    df[\"source\"] = np.array([tweet.source for tweet in tweets])\n",
    "    df[\"likes\"] = np.array([tweet.favorite_count for tweet in tweets])\n",
    "    df[\"retweets\"] = np.array([tweet.retweet_count for tweet in tweets])\n",
    "\n",
    "    return df\n",
    "\n",
    "# add hashtags in the following list\n",
    "hashtags = [\n",
    "'#kilimall', \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tweets = 0\n",
    "\n",
    "\"\"\"\n",
    "The following for loop will collect a tweets that have the hashtags\n",
    " mentioned in the list and save the tweets into csv file\n",
    "\"\"\"\n",
    "\n",
    "for n in tqdm(hashtags):\n",
    "    # first we fetch all tweets that have specific hashtag\n",
    "    tweets = tweetSearch(n, 100)\n",
    "    # then we convert the tweets into pandas data frame\n",
    "    total_tweets += int(len(tweets))\n",
    "    df = tweets_to_data_frame(tweets)\n",
    "    # then we save the data frame into csv file\n",
    "    df.to_csv(\"data/{}.csv\".format(n), index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
